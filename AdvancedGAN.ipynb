{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo0V7ZsisGOCDTL0OnfKsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysnarslan/GAN/blob/main/AdvancedGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y8wGpg8r-vfa"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch, torchvision, os, PIL, pdb\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show(tensor, num=25, wandbactive=0, name=''):\n",
        "    data = tensor.deatch()\n",
        "    grid = make_grid(data[:num], nrow=5).permute(1, 2, 0)\n",
        "\n",
        "    if(wandbactive and wandb):\n",
        "        wandbactive.log({name: wandbactive.Image(grid.numpy().clip(0, 1))})\n",
        "\n",
        "    plt.imshow(grid).clip(0, 1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "j_f2oc26ATzr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters & General parameters\n",
        "n_epoch = 100\n",
        "batch_size = 128\n",
        "lr = 1e-4\n",
        "z_dim = 200\n",
        "device = 'cuda' # GPU\n",
        "\n",
        "cur_step = 0\n",
        "crit_cycles = 5\n",
        "gen_losses = []\n",
        "crit_losses = []\n",
        "show_step = 35\n",
        "save_step = 35\n",
        "wandb = 1 # Track in wandb, optinal (Track: 1, Not track: 0)"
      ],
      "metadata": {
        "id": "JEtRGIVfDJkT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optional\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login(key='') # Enter your API key."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O33BYmwYFGMn",
        "outputId": "21b5a042-1ec4-4e5b-81f4-8c8522aee40a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "experiment_name = wandb.util.generate_id()\n",
        "\n",
        "myrun = wandb.init(\n",
        "    project = 'wgan',\n",
        "    group = experiment_name,\n",
        "    config = {\n",
        "        'optimizer': 'adam',\n",
        "        'model': 'wgan gp',\n",
        "        'batch_size': 128,\n",
        "        'epoch': 1000,\n",
        "    }\n",
        ")\n",
        "\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "ZtRuk0H9F8V1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka0VflgpG9ds",
        "outputId": "e3e4290b-cc4d-488c-8d5a-5a96797e5511"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bqsqnsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=64, d_dim=16):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.gen = nn.Sequential(\n",
        "            # inp = 1x1x200 (x_dim) -> out = 4x4x512 (d_dim * 32)\n",
        "            nn.ConvTranspose2d(in_channels = z_dim,\n",
        "                               out_channels = d_dim*32,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 1,\n",
        "                               padding = 0),\n",
        "            nn.BatchNorm2d(d_dim * 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 4x4x512 -> 8x8x256\n",
        "            nn.ConvTranspose2d(in_channels = d_dim*32,\n",
        "                               out_channels = d_dim*16,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 2,\n",
        "                               padding = 1),\n",
        "            nn.BatchNorm2d(d_dim * 16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # in = 8x8x256 -> out = 16x16x128\n",
        "            nn.ConvTranspose2d(in_channels = d_dim * 16,\n",
        "                               out_channels = d_dim * 8,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 2,\n",
        "                               padding = 1),\n",
        "            nn.BatchNorm2d(d_dim * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # in = 16x16x128 -> out = 32x32x64\n",
        "            nn.ConvTranspose2d(in_channels = d_dim * 8,\n",
        "                               out_channels = d_dim * 4,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 2,\n",
        "                               padding = 1),\n",
        "            nn.BatchNorm2d(d_dim * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # in = 32x32x64 -> out = 64x64x32\n",
        "            nn.ConvTranspose2d(in_channels = d_dim * 4,\n",
        "                               out_channels = d_dim * 2,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 2,\n",
        "                               padding = 1),\n",
        "            nn.BatchNorm2d(d_dim * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels = d_dim * 2,\n",
        "                               out_channels = 3,\n",
        "                               kernel_size = 4,\n",
        "                               stride = 2,\n",
        "                               padding = 1),\n",
        "            # Produce result in the range from -1 to 1\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1) # 128x200x1x1\n",
        "        return self.gen(x)\n",
        "\n",
        "def gen_noise(num, z_dim, device='cuda'):\n",
        "    return torch.randn(num, z_dim, device=device) # 128x200"
      ],
      "metadata": {
        "id": "BPGWe2OeLTTZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(z_dim = 200)\n",
        "\n",
        "gen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goJSfssIAO4t",
        "outputId": "0d9d321c-d8db-404b-fe7d-dfbee2a459e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (gen): Sequential(\n",
              "    (0): ConvTranspose2d(200, 512, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (16): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}